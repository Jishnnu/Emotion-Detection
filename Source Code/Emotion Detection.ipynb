{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieZpkwlrqh8o"
      },
      "source": [
        "# **CASE STUDY : DEEP LEARNING**\n",
        "We briefly touched upon the libraries that python offers machine learning in chapter 2. Let's take a hands-on approach to deepen our understanding of these concepts.\n",
        "\n",
        "Machine learning's subfield, Emotion Detection, includes evaluating and identifying human emotions from a variety of sources, including text, audio, facial expressions, and physiological signs. Psychology, healthcare, customer service, and marketing are just a few of the industries where emotion detection has applications.\n",
        "\n",
        "On labelled data sets, machine learning algorithms are taught to identify patterns in the input data that correspond to various emotional states. For instance, algorithms for face recognition may be trained to recognise expressions that are indicative of various emotions, such as surprise, rage, grief, or happiness. Here we are particulary interested in exploring the dataset using a deep learning model.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Deep Learning**\n",
        "In order to learn hierarchical representations of data, deep learning entails training artificial neural networks with several layers. The ability of these deep neural networks to autonomously learn and extract complex characteristics from high-dimensional data enables them to achieve state-of-the-art performance on a number of tasks, including speech and picture recognition, natural language processing, and game playing.\n",
        "\n",
        "<br>\n",
        "\n",
        "### Dataset : \n",
        "We are using an image-based dataset that shows 7 different human emotions - Anger, Disgust, Fear, Happiness, Neutral, Sadness, and Surprise.\n",
        "\n",
        "You can download the dataset from https://www.kaggle.com/datasets/jonathanoheix/face-expression-recognition-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nT0osh7YafO6",
        "outputId": "ee3f1200-de5a-4994-c622-92a9398354f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eT1IE6CyRBe"
      },
      "source": [
        "## **CONVOLUTIONAL NEURAL NETWORK**\n",
        "The code in the following cells define the architecture of a Convolutional Neural Network (CNN) model for human emotion detection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xohkRJHhoaF"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7R45xQe5iGzL"
      },
      "outputs": [],
      "source": [
        "# Setting the paths for our training and validation data:\n",
        "\n",
        "folder = \"/content/drive/MyDrive/Emotion_Detection/images\"  # This is my custom path, and it may be different from your folder/file path\n",
        "train_data_path = os.path.join(folder, \"train\")\n",
        "test_data_path = os.path.join(folder, \"validation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFkjeM7Oo917"
      },
      "outputs": [],
      "source": [
        "# Create data generators for our train and validation datasets\n",
        "\n",
        "batch_size = 32\n",
        "img_size = (48, 48)\n",
        "\n",
        "# The following lines of code define an ImageDataGenerator object for data augmentation during training of a neural network for image classification.\n",
        "train_datagen = ImageDataGenerator(    \n",
        "    rescale=1./255,                       # Scales the pixel values of the image to be between 0 and 1.\n",
        "    rotation_range=20,                    # Randomly rotates the image by a specified number of degrees in the given range.\n",
        "    zoom_range=0.2,                       # Randomly zooms into the image by a specified factor in the given range.\n",
        "    width_shift_range=0.2,                # Randomly shifts the image horizontally by a specified fraction of the total image size.\n",
        "    height_shift_range=0.2,               # Randomly shifts the image vertically  by a specified fraction of the total image size.\n",
        "    horizontal_flip=True,                 # Determines how the empty space created by the above transformations is filled.\n",
        "    fill_mode='nearest'                   # Fills it with the nearest pixel value.\n",
        ")\n",
        "\n",
        "# The next line of code defines an ImageDataGenerator object for data augmentation during validation of a neural network for image classification.\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\"\"\" \n",
        "The following lines of code create a data generator for the training and validation datasets, which can be used to load images in batches during model training.\n",
        "The train_datagen object's flow_from_directory method requests the directory path containing the training images, the goal size for the images, the batch size \n",
        "for loading the images, the colour mode (in this case, grayscale), and the class mode (categorical in this case). When a model is being trained, it returns a \n",
        "generator that can be used to load batches of photos and the labels that go with them. \"\"\"\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_data_path,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    color_mode='grayscale',\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    test_data_path,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    color_mode='grayscale',\n",
        "    class_mode='categorical'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJ0HpqpbqWKf"
      },
      "source": [
        "### **HYPER-PARAMETER DESCRIPTION**\n",
        "\n",
        "**ACTIVATION** <br> \n",
        "relu :The rectified linear activation function or ReLU for short is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero. It has become the default activation function for many types of neural networks because a model that uses it is easier to train and often achieves better performance.\n",
        "\n",
        "<br>\n",
        "\n",
        "**MODEL** <br>\n",
        "sequential : A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor.\n",
        "\n",
        "<br>\n",
        "\n",
        "**MAXPOOLING** <br> Maximum pooling, or max pooling, is a pooling operation that calculates the maximum, or largest, value in each patch of each feature map. The results are down sampled or pooled feature maps that highlight the most present feature in the patch, not the average presence of the feature in the case of average pooling.\n",
        "\n",
        "<br>\n",
        "\n",
        "**PADDING** <br>\n",
        "The padding parameter of the Keras Conv2D class can take one of two values: 'valid' or 'same'. Setting the value to “valid” parameter means that the input volume is not zero-padded and the spatial dimensions are allowed to reduce via the natural application of convolution.\n",
        "\n",
        "<br>\n",
        "\n",
        "**BATCH NORMALIZATION** <br> Batch normalization is a technique for training very deep neural networks that standardizes the inputs to a layer for each mini-batch. This has the effect of stabilizing the learning process and dramatically reducing the number of training epochs required to train deep networks.\n",
        "\n",
        "<br>\n",
        "\n",
        "**DROPOUT** <br> Dropout is a technique used to prevent a model from overfitting. Dropout works by randomly setting the outgoing edges of hidden units (neurons that make up hidden layers) to 0 at each update of the training phase.\n",
        "\n",
        "<br>\n",
        "\n",
        "**ADAM** <br> Adam can be looked at as a combination of RMSprop and Stochastic Gradient Descent with momentum. It uses the squared gradients to scale the learning rate like RMSprop and it takes advantage of momentum by using moving average of the gradient instead of gradient itself like SGD with momentum.\n",
        "\n",
        "<br>\n",
        "\n",
        "**SGD** <br> Stochastic Gradient Descent (SGD) addresses both of these issues by following the negative gradient of the objective after seeing only a single or a few training examples. The use of SGD In the neural network setting is motivated by the high cost of running back propagation over the full training set\n",
        "\n",
        "<br>\n",
        "\n",
        "**RMSprop** <br> RMSprop is a gradient based optimization technique used in training neural networks. ... This normalization balances the step size (momentum), decreasing the step for large gradients to avoid exploding, and increasing the step for small gradients to avoid vanishing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kxBmxPvpLrB"
      },
      "outputs": [],
      "source": [
        "# DEFINE CNN MODEL \n",
        "\n",
        "# An empty sequential model is created in the first line, to which successive layers are added.\n",
        "model = Sequential()\n",
        "\n",
        "\"\"\" \n",
        "* The next lines add Convolutional layers to the model with increasing depth and reducing spatial dimensions of the feature maps through MaxPooling layers.\n",
        "* The Rectified Linear Unit (ReLU), which is well known for performing well in image recognition tasks, is the activation function utilized in all convolutional layers.\n",
        "* When the padding is set to \"same,\" zeros are appended to the input to provide the output the same spatial dimensions as the input.\n",
        "* Each Conv2D layer is followed by a layer of BatchNormalization to normalize the activations from the preceding layer.\n",
        "* To minimize overfitting, dropout layers are introduced after each MaxPooling layer.\n",
        "\"\"\"\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(img_size[0], img_size[1], 1)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Convolutional layer output is transformed into a one-dimensional vector by the Flatten layer, which can then be fed into a fully linked layer.\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "\"\"\" \n",
        "To extract the probability distribution of 7 emotions, 2 dense layers are added at the end, one with 128 units and ReLU activation (mentioned above) and \n",
        "the other with 7 units and softmax activation.\n",
        "\"\"\"\n",
        "model.add(Dense(7, activation='softmax'))\n",
        "\n",
        "# To print the model's architecture, including the number of parameters in each tier, the summary method is invoked.\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjsPIPfopP5t"
      },
      "outputs": [],
      "source": [
        "\"\"\" \n",
        "COMPILE AND TRAIN THE MODEL\n",
        "\n",
        "The model is set up for training using the model.compile() function. In this line of code, we are building the model using the Adam optimizer, a well-liked \n",
        "stochastic gradient descent optimizer. Moreover, categorical cross-entropy, which is frequently employed for multiclass classification issues, is the loss \n",
        "function that we have specified. Finally, we define accuracy as the statistic that will be used to assess the model's performance throughout training.\n",
        "\n",
        "The model is trained using the model.fit() function. We are fitting the model to our training data by using train_generator as the input data, \n",
        "train_generator.samples // batch_size as the number of steps_per_epoch (the number of batches of samples to use in each epoch), 50 as the number of epochs, \n",
        "val_generator as the validation data, and val_generator.samples // batch_size as the number of validation steps. The model will be tuned during training to \n",
        "reduce category cross-entropy loss and increase the accuracy metric. The history object contains the training history.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // batch_size,\n",
        "    epochs=45,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=val_generator.samples // batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGNj8easpUbd"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "EVALUATING THE MODEL AND PLOTTING A GRAPH TO VISUALIZE THE ACCURACY & LOSS\n",
        "\n",
        "These lines of code create two graphs, one for the accuracy of a machine learning model during training and validation and the other for the \n",
        "loss during training and validation, using the Python module Matplotlib. Here is a detailed explanation of the code:\n",
        "\n",
        "\"\"\"\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "\n",
        "# This line displays the values from the training and validation sets over the epochs that were recorded in the history object and correspond to the input parameters.\n",
        "plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
        "\n",
        "# This line changes the graph's title\n",
        "plt.title('Training and validation accuracy') \n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "\n",
        "# This line changes the graph's title\n",
        "plt.title('Training and validation loss') \n",
        "\n",
        "# This line adds a legend to the graph\n",
        "plt.legend() \n",
        "\n",
        "# This line displays the graph.\n",
        "plt.show() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIb9Ew-NdXQt"
      },
      "source": [
        "Now let's use OpenCV to test out how our model would behave to video inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NVrO4IedgNH"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "model.save('emotion_detection_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwTcpDhEdl_b"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the trained facial emotion detection model\n",
        "model = load_model('/content/emotion_detection_model.h5') # This is my cusstom path, and it may be different from your folder/file path\n",
        "\n",
        "# Define a dictionary to map emotion labels to their names\n",
        "emotions = {\n",
        "    0: 'angry',\n",
        "    1: 'disgust',\n",
        "    2: 'fear',\n",
        "    3: 'happy',\n",
        "    4: 'neutral',\n",
        "    5: 'sad',\n",
        "    6: 'surprise'\n",
        "}\n",
        "\n",
        "# Create a VideoCapture object to capture the video stream\n",
        "cap = cv2.VideoCapture('/content/drive/MyDrive/Emotion_Detection/Emotions.mp4') # This is my custom path, and it may be different from your file path\n",
        "\n",
        "# Define the face detection model\n",
        "face_cascade = cv2.CascadeClassifier('/content/drive/MyDrive/Emotion_Detection/haarcascade_frontalface_default.xml') # This is my custom path, and it may be different from your file path\n",
        "\n",
        "# Define the output video codec and frame rate\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "# Define the output video writer\n",
        "out = cv2.VideoWriter('output.mp4', fourcc, fps, (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n",
        "\n",
        "# Loop through each frame in the video stream\n",
        "while cap.isOpened():\n",
        "    # Read the next frame from the video stream\n",
        "    ret, frame = cap.read()\n",
        "    \n",
        "    # If there's an error reading the frame, break out of the loop\n",
        "    if not ret:\n",
        "        break\n",
        "        \n",
        "    # Convert the frame to grayscale for face detection\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    \n",
        "    # Detect faces in the frame using the face detection model\n",
        "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30), flags=cv2.CASCADE_SCALE_IMAGE)\n",
        "    \n",
        "    # For each face detected, predict the emotion using the trained model\n",
        "    for (x, y, w, h) in faces:\n",
        "        # Crop the face region from the frame\n",
        "        face_image = gray[y:y+h, x:x+w]\n",
        "        face_image = cv2.resize(face_image, (48, 48))\n",
        "        face_image = np.reshape(face_image, (1, 48, 48, 1))\n",
        "        \n",
        "        # Normalize the pixel values to be between 0 and 1\n",
        "        face_image = face_image / 255.0\n",
        "        \n",
        "        # Predict the emotion using the trained model\n",
        "        emotion_probabilities = model.predict(face_image)[0]\n",
        "        predicted_emotion = emotions[np.argmax(emotion_probabilities)]\n",
        "        \n",
        "        # Draw the predicted emotion label on the frame\n",
        "        cv2.putText(frame, predicted_emotion, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "        \n",
        "        # Draw a rectangle around the face on the frame\n",
        "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "    \n",
        "    # Write the frame to the output video\n",
        "    out.write(frame)\n",
        "\n",
        "    # Exit the loop if the user presses the 'q' key\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release the VideoCapture and VideoWriter objects\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "# Close all windows\n",
        "cv2.destroyAllWindows()    "
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
